n_runs: 50
n_splits: 5
learning_rate: 0.0007321501429927418
train_batch_size: 512
val_batch_size: 64
train_steps: 2000
pooling_mode: max
raw_train_path: /Users/bas/PycharmProjects/jarvis2/data/20231106_105236_dip_train_2023_2_0_20x300_23432.csv
raw_test_path: /Users/bas/PycharmProjects/jarvis2/data/20230927_172709_eqk_validation_0_20x300_800.csv
#n_workers: 5
val_steps: 100
hidden_dim:
  low: 128
  high: 512
  int: True
readout_dim: 128
n_ffn_blocks_emb:
  low: 1
  high: 3
  int: True
n_ffn_blocks_readout:
  low: 1
  high: 3
  int: True
cache_embeddings: True
exp_name: parameter_count
dropout_rate: 0.15
loss_fn: cosine
es_delta: 0.001
allow_half_label: True
es_patience: 10
#model_name: distilroberta-base
#model_name: sentence-transformers/all-mpnet-base-v2
#remove_synonym_skills:
#  - True
#  - False
#remove_strange_skills:
#  - True
#  - False
#rename_skills:
#  - True
#  - False